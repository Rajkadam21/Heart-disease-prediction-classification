# -*- coding: utf-8 -*-
"""Heart Disease Prediction .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QQ_sNF8aqvVUPFbZ1_aufmy2AqVX5V91
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

df=pd.read_csv("Heart.csv")

df.head()

df.info()

df.describe()

df.isnull().sum()

sns.heatmap(df.isnull())

"""We have null values in column Ca and Thal

we have null values less than 3 percent so we can drop the null values
"""

df.drop(['Unnamed: 0'],axis=1,inplace=True)

df.dropna(how="all",subset=['Ca'],inplace=True)

df.dropna(how="all",subset=['Thal'],inplace=True)

sns.heatmap(df.isnull())

"""#Data Visualization"""

#bivariant analysis
sns.barplot(x="AHD",y="Sex",data=df)
plt.title("Target VS Sex")
plt.show()

countFemale = len(df[df.Sex == 0])
countMale = len(df[df.Sex == 1])
print("Percentage of Female Patients: {:.2f}%".format((countFemale / (len(df.Sex))*100)))
print("Percentage of Male Patients: {:.2f}%".format((countMale / (len(df.Sex))*100)))

pd.crosstab(df.Age,df.AHD).plot(kind="bar",figsize=(20,6))
plt.title('Heart Disease Frequency for Ages')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

sns.kdeplot(df['Oldpeak'],color='red',shade=True)

pd.crosstab(df.Sex,df.AHD).plot(kind="bar",figsize=(15,6))
plt.title('Heart Disease Frequency for Sex')
plt.xlabel('Sex (0 = Female, 1 = Male)')
plt.legend(["Haven't Disease", "Have Disease"])
plt.ylabel('Frequency')
plt.show()

df.hist(figsize=(12,12))
plt.show()

#univariant analysis
con=(df['Chol']>=350)
sns.countplot(x='Chol', data= df[con])
plt.show()

sns.countplot(x='AHD', data= df)
plt.show()

sns.countplot(x='Ca', data= df)
plt.show()

#multivariant analysis
sns.pairplot(df)

"""##Handling Outliers"""

numcol=[]
catcol=[]
for i in df.dtypes.index:
  if df.dtypes[i]=="object":
    catcol.append(i)
  else:
    numcol.append(i)

numcol

catcol

"""#Plot boxplot for numeric values for find out outliers from column

"""

plt.figure(figsize=(10,10))
pltn=1
for i in numcol:
    if pltn<=12:
        ax=plt.subplot(6,2,pltn)
        sns.boxplot(df[i])
        plt.xlabel(i)
        pltn=pltn+1
        plt.show()
plt.tight_layout()

"""Outliers can be handled by
1. IQR +- 1.5
2. IF Z-SCORE IS > 3 remove the outlier

IQR method causes more dataloss than zscore therefore We use z-score for removing outliers
"""

features=df[['Age','Sex','RestBP','Chol','Fbs','RestECG','MaxHR','ExAng','Oldpeak','Slope','Ca']]

#To import z score we use
from scipy.stats import zscore
z=abs(zscore(features))

z

newdf =df[(z<=3).all(axis =1)] #axis=1(columns),axis =0(rows)

newdf.shape

df.shape

dataloss= (297-288)/297
dataloss*100

"""# Handling Skewness"""

df.skew()

newdf.skew()

"""Our target is in the categorical form so, we have to encode it with the label encoder."""

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
newdf['AHD']=le.fit_transform(newdf['AHD'])

newdf

"""Find out correlation of feature and Target using Heatmap."""

plt.figure(figsize=(20,15))
sns.heatmap(newdf.corr(),annot=True)

"""If skewness of columns with n correlation with target is >0.5 then remove skewness of those columns.

From heatmap we conclude that we have skewness in Column Unnamed:0,Age,Sex,
RestBP,Chol,Fbs,RestECG,MaxHR,ExAng,Oldpeak,Slope,Ca.
"""

skew1 = ['Age','Sex','RestBP','Chol','Fbs','RestECG','MaxHR','ExAng','Oldpeak','Slope','Ca']
from sklearn.preprocessing import PowerTransformer
pt=PowerTransformer(method ="yeo-johnson")

newdf[skew1]=pt.fit_transform(newdf[skew1].values)

newdf.head()

"""#Feature Encoding

```
# This is formatted as code
```

For feature Encoding we use OrdinalEncoder()
"""

from sklearn.preprocessing import OrdinalEncoder
oe =OrdinalEncoder()
newdf[catcol] = oe.fit_transform(newdf[catcol])

newdf.head()

"""#Feature Scaling

Only done on features and not on labels.

We use StandardScalar for scaling.



"""

x=newdf.drop("AHD", axis =1)
y=newdf["AHD"]

x

y

from sklearn.preprocessing import StandardScaler
sc =StandardScaler()
x= sc.fit_transform(x)
x

"""#Train Test Spilt"""

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.4,random_state =1)

from sklearn.metrics import classification_report,accuracy_score,confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold, cross_val_score
from sklearn.model_selection import GridSearchCV

def mymodel(model):
    model.fit(xtrain,ytrain)
    ypred=model.predict(xtest)
    train=model.score(xtrain,ytrain)
    test=model.score(xtest,ytest)

    print(f"Traning accuracy:{train}\n Testing accuracy:{test}\n\n")
    print(confusion_matrix(ytest,ypred))
    print(classification_report(ytest,ypred))
    print(accuracy_score(ytest,ypred))
    return model

lr=mymodel(LogisticRegression())
cvs = cross_val_score(lr, x, y, cv=5, scoring="accuracy")
print(f"Avg.Accuracy : {cvs.mean()}\nSTD : {cvs.std()}")

knn=mymodel(KNeighborsClassifier())
cvs = cross_val_score(knn, x, y, cv=5, scoring="accuracy")
print(f"Avg.Accuracy : {cvs.mean()}\nSTD : {cvs.std()}")

bnb=mymodel(BernoulliNB(alpha=10,fit_prior=True,class_prior=None))
cvs = cross_val_score(bnb, x, y, cv=5, scoring="accuracy")
print(f"Avg.Accuracy : {cvs.mean()}\nSTD : {cvs.std()}")

Gb=mymodel(GaussianNB())
cvs = cross_val_score(Gb, x, y, cv=5, scoring="accuracy")
print(f"Avg.Accuracy : {cvs.mean()}\nSTD : {cvs.std()}")

dc=mymodel(DecisionTreeClassifier())
cvs = cross_val_score(dc, x, y, cv=5, scoring="accuracy")
print(f"Avg.Accuracy : {cvs.mean()}\nSTD : {cvs.std()}")

XGB=mymodel(XGBClassifier(random_state=1,reg_alpha=1))
cvs = cross_val_score(XGB, x, y, cv=5, scoring="accuracy")
print(f"Avg.Accuracy : {cvs.mean()}\nSTD : {cvs.std()}")

Ada=mymodel(AdaBoostClassifier(random_state=1))
cvs = cross_val_score(Ada, x, y, cv=5, scoring="accuracy")
print(f"Avg.Accuracy : {cvs.mean()}\nSTD : {cvs.std()}")

lsvc=mymodel(LinearSVC(random_state=1))
cvs = cross_val_score(lsvc, x, y, cv=5, scoring="accuracy")
print(f"Avg.Accuracy : {cvs.mean()}\nSTD : {cvs.std()}")

Rc=mymodel(RandomForestClassifier())
cvs = cross_val_score(Rc, x, y, cv=5, scoring="accuracy")
print(f"Avg.Accuracy : {cvs.mean()}\nSTD : {cvs.std()}")

"""#Hyperparameter Tunning for BernoulliNB

"""

#gridsearch CV
 params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0],
          'fit_prior': [True, False],
          #'class_prior': [None, [0.1,]* len(n_classes),],
          'binarize': [None, 0.0, 8.5, 10.0]}

grid=GridSearchCV(BernoulliNB(),params)
grid.fit(xtrain,ytrain)

grid.best_params_

bnb=mymodel(BernoulliNB(alpha=0.01,binarize=0.0,fit_prior=True,class_prior=None))
#cvs = cross_val_score(bnb, x, y, cv=5, scoring="accuracy")
print(f"Avg.Accuracy : {cvs.mean()}\nSTD : {cvs.std()}")

"""##Hyperparameter Tunning for GaussianNB"""

#grid searchcv
params = {
            #'priors': [None, [0.1,]* len(n_classes),],
            'var_smoothing': [1e-9, 1e-6, 1e-12],
         }

grid=GridSearchCV(GaussianNB(),param_grid=params, n_jobs=-1, cv=5, verbose=5)
grid.fit(xtrain,ytrain)

grid.best_params_

bnb=mymodel(GaussianNB(priors=None,var_smoothing=1e-09))
cvs = cross_val_score(bnb, x, y, cv=5, scoring="accuracy")
print(f"Avg.Accuracy : {cvs.mean()}\nSTD : {cvs.std()}")

newdf

df

Data contains;

Age - age in years

Sex - (1 = male; 0 = female)

ChestPain - chest pain type

RestBP - resting blood pressure (in mm Hg on admission to the hospital)

Chol - serum cholestoral in mg/dl

Fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)

RestECG - resting electrocardiographic results

MaxHR - maximum heart rate achieved

ExAng - exercise induced angina (1 = yes; 0 = no)

Oldpeak - ST depression induced by exercise relative to rest

Slope - the slope of the peak exercise ST segment

Ca - number of major vessels (0-3) colored by flourosopy

Thal - 3 = normal; 6 = fixed defect; 7 = reversable defect

AHD - have disease or not (1=yes, 0=no)

df['Ca'].value_counts()

df['Slope'].value_counts()

df['ExAng'].value_counts()

df['Fbs'].value_counts()

df['RestECG'].value_counts()

